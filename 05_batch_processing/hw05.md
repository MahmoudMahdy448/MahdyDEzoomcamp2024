# Homework Solution for Week 5

This week's homework involved putting what we learned about Spark into practice. Here's a breakdown of how each question was solved:

## Question 1: Install Spark and PySpark
Installed Spark and PySpark, created a local spark session, and executed spark.version. The output was **'3.5.0'**. Here's the code snippet:
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("local_hw05_question1") \
    .getOrCreate()
print("Spark Version:", spark.version)
```

## Question 2: FHV October 2019
Read the October 2019 FHV data into a Spark Dataframe with a schema. Repartitioned the Dataframe to 6 partitions and saved it to parquet. The average size of the Parquet files was **6MB**. Here’s a snippet of the code:

```python
df = spark.read \
    .option("header", "true") \
    .csv('data/raw/fhv/2019/10/fhv_tripdata_2019_10.csv')
df = df.repartition(6)
df.write.parquet('data/parquet/fhv/2019/10', mode = 'overwrite')
```

## Question 3: Count records
Counted the number of taxi trips that started on the 15th of October. The total count was **62,610**. Here’s the code snippet:

```python
from pyspark.sql import functions as F
filtered_df = df \
    .filter(F.to_date(df.pickup_datetime) == '2019-10-15')
filtered_df.count()

```


## Question 4: Longest trip for each day
Calculated the length of the longest trip in the dataset in hours. The longest trip was **631,152.50 hours** . Here’s the code snippet:

```python
df = df.withColumn('hours_diff', (F.unix_timestamp(df.dropOff_datetime) - F.unix_timestamp(df.pickup_datetime)) / 3600)
longest_trip = df.groupby().max('hours_diff').first().asDict()['max(hours_diff)']
```


## Question 5: User Interface
Spark’s User Interface which shows the application's dashboard runs on local **port 4040**.


## Question 6: Least frequent pickup location zone
Loaded the zone lookup data into a temp view in Spark. Using the zone lookup data and the FHV October 2019 data, found that the name of the LEAST frequent pickup location Zone was **'Jamaica Bay'**. Here’s the code snippet:

```python
df_zones = spark.read \
    .option("header", "true") \
    .csv('data/taxi_zone_lookup.csv')
df_join = df \
    .join(df_zones, df.PUlocationID == df_zones.LocationID)
least_frequent_pickup_zone = df_join.groupby('Zone').count().orderBy(F.col('count').asc()).first().asDict()['Zone']
```


